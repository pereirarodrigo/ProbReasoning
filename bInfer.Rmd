```{r echo = FALSE}
library(distill)
```

---
title: "Bayesian inference in computational intelligence"
author: 
  - name: "Rodrigo Pereira Cruz"
    email: pereirarodrigocs@gmail.com
output:
  html_document:
    df_print: paged
---

<link rel="stylesheet" type="text/css" href="styles.css"/>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---

> This is a notebook used for studying Bayesian inference in computational intelligence, which is focused on its theoretical applications. The R programming language was chosen for the practical part of this project, as its orientation towards statistical analysis, ease of use and modular nature makes it well suited for this kind of task.  

---

# 1. Introduction

***

Bayes' theorem is a fundamental part of probability and statistics. It is deeply tied to conditional probability, often expressed as $P(A | B)$ and read as the probability of A given B. Bayesian statistics relies on having prior probabilities, which constitutes the currently known information, and calculating posterior probabilities based on the prior and likelihood. This is a constant process, where the prior probabilities are constantly updated. Bayes' theorem is usually defined as

<br>

$$P(A | B) = \frac{P(B | A) \space P(A)}{P(B)}$$

<br>

where

<br>

* $P(A | B)$ = posterior probability;
* $P(B | A)$ = likelihood;
* $P(A)$ = prior probability;
* $P(B)$ = marginal probability.

<br>

Oftentimes, this definition is expanded upon when $P(B)$ isn't clearly defined:

<br>

$$\begin{align}
    P(A | B) &= \frac{P(B | A) \space P(A)}{P(B)} \\
             &= \space \frac{P(B | A) \space P(A)}{P(B | A) \space P(A) + P(B | \neg A) \space P(\neg A)}
\end{align}$$

<br>

When it comes to computing, Bayes' theorem is already popular in the field of machine learning, where it's known as the naive Bayes algorithm and is used as a classifier. A simple usage can be seen below:

```{r}

# A simple use of the naive Bayes model for classification
# The model efficiently classifies 3 types of iris flowers: setosa, versicolor and virginica

library(e1071)
library(caret)

data("iris")

iris_dataset <- as.data.frame(iris)
bayes <- naiveBayes(Species ~., data = iris_dataset)

# Check the model
bayes

bayes_predictions <- predict(bayes, iris_dataset)

# Checking the predictions, the confusion matrix and the accuracy

table(bayes_predictions, iris_dataset$Species)

confusionMatrix(iris$Species, bayes_predictions)
```
---

# 2. Continuous random variables

***

Although the usage of probability mass functions (PMFs) is common when dealing with probability distributions, Bayesian inference often deals with continuous variables, which requires the use of probability density functions (PDFs).

When it comes to PDFs, it's impossible to use a precise probability value for the event we're interested in - calculating the **exact** probability of raining exactly 2mm in a week, for example, is impractical. When situations like this arise, the need of using ranges for our values is made absolutely necessary and, thus, dealing with continuous variables becomes a matter of calculating the area between inferior and superior ranges. Mathematically, this can be expressed as

<br>

$$P(a \leq X \leq b) = \int_{a}^{b} f_X(x)dx = F(b) - F(a) $$

<br>

This is, of course, only a general overview, as some of the most common continuous distributions - normal, uniform, beta and gamma - are different and are used for different tasks. However, what all of them share in Bayesian statistics is the presence of prior elicitation, which is a belief in one's data distribution. For Bayesians, this elicitation expresses their belief in terms of personal probabilities and must adhere to the laws or probability (e.g. having a probability of 150% of winning a lottery is invalid for a prior elicitation). Thus, given a Bayesian has this belief in their distribution, we must deal with knowing the probability of success of that data.

In a distribution, the probability of success, known as $p$, is not usually known. For example, when tossing a coin, the probability of heads is, for a balanced coin, 0.5 or $\frac{1}{2}$. However, we can't know if a coin is balanced, thus we **predict** that this value, our $p$, must be closer to 0.5 than 0 or 1. A Bayesian will express their belief in a particular value of $p$ through a probability distribution, with the **beta family** being a popular choice, usually being defined, mathematically, as

<br>

$$p \sim beta(\alpha, \beta)$$

<br>

whose PDF is

<br>

$$f(p) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)  \Gamma(\beta)} p^{\alpha - 1} (1 - p)^{\beta - 1}$$

<br>

where $0 \leq p \leq 1, \alpha > 0, \beta > 0$ and $\Gamma$ is a factorial:

<br>

$$\Gamma(n) = (n - 1)! = (n - 1) \times (n - 2) \times ... \times 1$$

<br>

where $n$ is the number of trials.

<br>

When it comes to computing, it's possible to write a very simple R script that generates a beta distribution, with multiple options being available. An example is shown below:

```{r}
# Generating a beta distribution out of a sequence of values

values <- seq(0, 1, 0.02)

# dbeta gives us a PDF
# shape1 = alpha and shape2 = beta

beta_dist <- dbeta(values, shape1 = 2, shape2 = 3)

# Plotting the results

plot(beta_dist, 
     main = "A beta distribution",
     xlab = "x", 
     ylab = "y",
     col = "blue",
     type = "o")
```

---

# 3. Bayesian decision-making and loss functions

***

The posterior probability is the basis of any Bayesian inference, as it's essentially updated knowledge given it incorporates prior information with new data. However, we might want to express a credible interval as our inference, which makes using posterior distributions difficult - because of this, we are now dealing with decision theory and, thus, must make use of loss functions for optimization. There are usually three loss functions that are used and are minimized at specific points, which are:

<br>

* **Linear** = uses a median as its best estimate (minimized point);
* **Quadratic** = uses a mean as its best estimate;
* **0/1** = uses a mode as its best estimate.

<br>

Starting with 0/1 loss, the goal is to check if each value in the posterior distribution is equal to, or different than, an estimate. Should the value be equal to your estimate, there will be no loss; otherwise, a loss of 1 point will happen. Mathematically, this function is defined as

<br>

$$L_{0, i}(0, g) = \begin{cases}
                      0 & if \space g = x_i \\
                      1 & otherwise
                   \end{cases}$$
                   
<br>

where $g$ is our guess or an estimate. The total loss is defined as

<br>

$$L_0 = \sum_i L_{0, i}(0, g)$$

<br>

A quick example is given below, where we have some random data, our estimate is 12 and we're using the 0/1 loss:

<br>

<center> **Table 1:** $L_0$: 0/1 for estimate 12 </center>

<br>


|$i$ |$x_i$  |$L_0: 0/1$
|:--:|:-----:|:-------:
| 1  |   2   |   1
| 2  |   6   |   1
| 3  |   3   |   1
| 4  |   12  |   0
| 5  |   14  |   1
| -  | **Total** |  **4**
 
<br>

Moving on to the linear loss function, it's defined, mathematically, as 

<br>

$$L_1(g) = \sum_i |x_i - g|$$

<br>

Note that we need absolute values here, given overestimates and underestimates do not cancel out. By calculating the median of some random data, and with our estimate being 6, we have

<br>

<center> **Table 2:** $L_1$: linear loss for estimate 6 </center>

<br>

|$i$ |$x_i$   |$L_1: |x_i - 6|$
|:--:|:------:|:--------------:
| 1  |   10   |   4
| 2  |   3    |   3
| 3  |   7    |   1
| 4  |   6    |   0
| 5  |   6    |   0
| -  | **Total** |  **8**
 
<br>

The squared loss function works similarly to the two previously mentioned loss functions, being defined, mathematically, as

<br>

$$L_2(g) = \sum_i (x_i - g)^2$$

<br>

By calculating the mean of some random data, and with our estimate being 7, we have

<br>

<center> **Table 3:** $L_2$: linear loss for estimate 7 </center>

<br>

|$i$ |$x_i$   |$L_2: (x_i - 7)^2$
|:--:|:------:|:--------------:
| 1  |   2    |   25
| 2  |   8    |   1
| 3  |   7    |   0
| 4  |   0    |   49
| 5  |   4    |   9
| -  | **Total** |  **84**
 
<br>

There is no single best option when it comes to loss functions, as each of them will point out to a different value of the distribution that is the optimized value: $L_0$ will point to the distribution's mode, while $L_1$ and $L_2$ will point to the median and mean, respectively. It will depend on which of them is the best fit for a particular report.

---

# 4. Minimizing expected loss for Bayesian hypothesis testing

***

Given that, in Bayesian statistics, the inference is based on the posterior distribution (as mentioned previously), we often make use of optimization when it comes to hypothesis testing.

<br>

> The following section includes personal notes and experiments that apply Bayesian hypothesis testing in computational intelligence and, despite having a similar structure and format, diverge significantly from the source material.

<br>

Let's assume that an intelligent agent has two competing hypotheses, $H_1$ and $H_2$, where

<br>

$P(H_1 \space is \space true \space | \space data)$ = posterior probability of $H_1$ <br>
$P(H_2 \space is \space true \space | \space data)$ = posterior probability of $H_2$

<br> 

Although there are different ways to choose between different hypotheses, considering a loss function is an interesting option given this is a decision problem. Let's suppose, for this section, that we're dealing with the following situation: an intelligent agent must decide whether it's in danger or not. Thus, we have:

<br>

$H_1$ = agent is not in danger; <br>
$H_2$ = agent is in danger.

<br>

Given those are mutually exclusive possibilities, only one of them will be realized. We can define our loss function as $L(d)$, with $d$ being a particular action. The agent's possible decisions are:

<br>

$d_1$: choose $H_1$: agent decides that it's in danger <br>
$d_2$: choose $H_2$: agent decides that it's not in danger 

<br>

Each of those decisions comes with a value $w$, which is a possible event that results from such a choice should the hypothesis be wrong. This value's impact differs from decision to decision and, for our agent in this scenario, they'd be the following:

<br>

For **$d = d_1$**, we have

* **Agent is correct:** the agent decides that it is not in danger, and it isn't. $\Rightarrow L(d_1) = 0$
  + **Agent is wrong:** the agent decides that it's not in danger, but it is. $\Rightarrow L(d_2) = w$

<br>

For **$d = d_2$**, we have

* **Agent is correct:** the agent decides that it's in danger, and it is. $\Rightarrow L(d_2) = 0$
  + **Agent is wrong:** the agent decides that it's in danger, but it isn't. $\Rightarrow L(d_2) = w$

<br>

Thus, we have two possible false negatives here when either $d_1$ or $d_2$ are wrong, which are:

<br>

**Wrong $d_1$ is a false negative**: the agent decides that it's not in danger when in reality it actually isn't <br>
**Possible results:** varied, but can range from an unpleasant situation to the agent being disabled permanently (severe outcome).

<br>

**Wrong $d_2$ is a false negative**: the agent decides that it's in danger when in reality it actually isn't <br>
**Possible results:** varied but, in general, may include some level of disturbance to its overall performance until its safety is assured (light outcome).

<br>

We can see straight away that $d_1$ being a false negative causes a much larger impact than $d_2$. Thus, using arbitrary numbers, we can define, mathematically, that

<br>

$$L(d_1) = \begin{cases}
                      0 & if \space d_1 \space is \space right \\
                      w_1 = 1000 & otherwise
            \end{cases}$$
            
<br>

$$L(d_2) = \begin{cases}
                      0 & if \space d_2 \space is \space right \\
                      w_2 = 50 & otherwise
            \end{cases}$$

<br>

For the final part of this example, lets imagine that our agent is an autonomous construction robot that's working on a particularly dangerous project. Thus, once again using arbitrary values, lets define our posterior probabilities:

<br>

$P(H_1 \space | \space p) \approx 0.35$ - posterior probability of the robot not being in danger while working on the project <br>
$P(H_2 \space | \space p) \approx 0.65$ - posterior probability of the robot being in danger while working

<br>

Thus, our expected losses would be

<br>

$E[L(d_1)] = 0.35 \times 0 + 0.65 \times 1000 = 650$ <br>
$E[L(d_2)] = 0.35 \times 50 + 0.65 \times 0 = 17.5$

<br>

The loss for $d_2$ is much lower than that of $d_1$, making it the robot's optimal choice in this situation. The following R script is used to illustrate this example in a more tangible way:

```{r}

# R code that returns the optimal choice between 2 decisions
# The posterior probabilities are used to calculate the expected loss

robot_choice <- function(d1, d2, p_prob){
  e_d1 <- numeric()
  e_d2 <- numeric()
  
  for(w in d1){
    e_d1 <- c(e_d1, w * Reduce("+", p_prob))
  }
  
  for(w in d2){
    e_d2 <- c(e_d2, w * Reduce("+", p_prob))
  }
  
  return (ifelse(which.max(e_d1) < which.max(e_d2), "d1", "d2"))
}

dec1 <- c(0, 50)
dec2 <- c(0, 1000)
prob <- c(0.35, 0.65)

print(paste0("The optimal choice is: ", robot_choice(dec1, dec2, prob)))
```
