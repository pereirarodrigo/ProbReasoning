```{r echo = FALSE}
library(distill)
```

---
title: "Bayesian inference in computational intelligence"
author: 
  - name: "Rodrigo Pereira Cruz"
    email: pereirarodrigocs@gmail.com
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---

This is a notebook used for studying Bayesian inference in computational intelligence, which is focused on its theoretical applications. The R programming language was chosen for the practical part of this project, given its orientation towards statistical analysis, ease of use and modular nature makes it well suited for this kind of task.  

---

# 1. Introduction

***

Bayes' theorem is a fundamental part of probability and statistics. It is deeply tied to conditional probability, often expressed as $P(A | B)$ and read as the probability of A given B. Bayesian statistics relies on having prior probabilities, which constitutes the currently known information, and calculating posterior probabilities based on the prior and likelihood. This is a constant process, where the prior probabilities are constantly updated. Bayes' theorem is usually defined as

<br>

$$P(A | B) = \frac{P(B | A) \space P(A)}{P(B)}$$

<br>

where

<br>

* $P(A | B)$ = posterior probability;
* $P(B | A)$ = likelihood;
* $P(A)$ = prior probability;
* $P(B)$ = marginal probability.

<br>

Oftentimes, this definition is expanded upon when $P(B)$ isn't clearly defined:

<br>

$$\begin{align}
    P(A | B) &= \frac{P(B | A) \space P(A)}{P(B)} \\
             &= \space \frac{P(B | A) \space P(A)}{P(B | A) \space P(A) + P(B | \neg A) \space P(\neg A)}
\end{align}$$

<br>

When it comes to computing, Bayes' theorem is already popular in the field of machine learning, where it's known as the naive Bayes algorithm and is used as a classifier. A simple usage can be seen below:

```{r}

# A simple use of the naive Bayes model for classification
# The model efficiently classifies 3 types of iris flowers: setosa, versicolor and virginica

library(e1071)
library(caret)

data("iris")

iris_dataset <- as.data.frame(iris)
bayes <- naiveBayes(Species ~., data = iris_dataset)

# Check the model
bayes

bayes_predictions <- predict(bayes, iris_dataset)

# Checking the predictions, the confusion matrix and the accuracy

table(bayes_predictions, iris_dataset$Species)

confusionMatrix(iris$Species, bayes_predictions)
```
---

# 2. Continuous random variables

***

Although the usage of probability mass functions (PMFs) is common when dealing with probability distributions, Bayesian inference often deals with continuous variables, which requires the use of probability density functions (PDFs).

When it comes to PDFs, it's impossible to use a precise probability value for the event we're interested in - calculating the **exact** probability of raining exactly 2mm in a week, for example, is impractical. When situations like this arise, the need of using ranges for our values is made absolutely necessary and, thus, dealing with continuous variables becomes a matter of calculating the area between inferior and superior ranges. Mathematically, this can be expressed as

<br>

$$P(a \leq X \leq b) = \int_{a}^{b} f_X(x)dx = F(b) - F(a) $$

<br>

This is, of course, only a general overview, as some of the most common continuous distributions - normal, uniform, beta and gamma - are different and are used for different tasks. However, what all of them share in Bayesian statistics is the presence of prior elicitation, which is a belief in one's data distribution. For bayesians, this elicitation expresses their belief in terms of personal probabilities and must adhere to the laws or probability (e.g. having a probability of 150% of winning a lottery is invalid for a prior elicitation). Thus, given a bayesian has this belief in their distribution, we must deal with knowing the probability of success of that data.

In a distribution, the probability of success, known as $p$, is not usually known. For example, when tossing a coin, the probability of heads is, for a balanced coin, 0.5 or $\frac{1}{2}$. However, we can't know if a coin is balanced, thus we **predict** that this value, our $p$, must be closer to 0.5 than 0 or 1. A bayesian will express their belief in a particular value of $p$ through a probability distribution, with the **beta family** being a popular choice, usually being defined, mathematically, as

<br>

$$p \sim beta(\alpha, \beta)$$

<br>

whose PDF is

<br>

$$f(p) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)  \Gamma(\beta)} p^{\alpha - 1} (1 - p)^{\beta - 1}$$

<br>

Where $0 \leq p \leq 1, \alpha > 0, \beta > 0$ and $\Gamma$ is a factorial:

---

# 3. Bayesian decision making and loss functions

***

